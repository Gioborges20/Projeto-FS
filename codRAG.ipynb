{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92232e28-f666-49fc-98f0-757cca6985a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T23:26:05.635981Z",
     "start_time": "2025-06-30T23:12:59.664518Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip \n",
    "%pip install -U langchain  \n",
    "%pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph  \n",
    "%pip install -qU \"langchain[mistralai]\" \n",
    "%pip install -qU langchain-huggingface\n",
    "%pip install sentence-transformers\n",
    "%pip install -qU langchain-core\n",
    "%pip install -qU langchain_community pypdf pillow\n",
    "%pip install hf_xet\n",
    "%pip install -qU langchain-chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0137f8e4-ce47-4172-8ee8-7d192410eee9",
   "metadata": {},
   "source": [
    "INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca185d467ce0d59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:32:17.095818Z",
     "start_time": "2025-06-30T22:32:17.091039Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"ANONYMIZED_TELEMETRY\"] = \"FALSE\"\n",
    "os.environ[\"CHROMA_TELEMETRY_ENABLED\"] = \"FALSE\"\n",
    "import getpass\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from typing import NamedTuple, Tuple\n",
    "from langgraph.graph import START, StateGraph\n",
    "from dotenv import load_dotenv,find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612dd9b-ad9b-4a98-bbb9-0447ecba22ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T22:46:50.764584Z",
     "start_time": "2025-06-30T22:46:47.211816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Langsmith e os e getpass para lidar com as chaves de api do .env\n",
    "load_dotenv(find_dotenv())  # Encontrar e carregar o documento .env que contém as chaves de api\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "if not os.environ.get(\"LANGSMITH_API_KEY\"):\n",
    "    os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass(\"Enter API key fot langsmith: \") # Se a chave de api não for encontrada o usuário deve inserir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51b92e-83f0-435d-b3c6-4adf0b04d934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T23:10:01.780171Z",
     "start_time": "2025-06-30T23:10:00.949797Z"
    }
   },
   "outputs": [],
   "source": [
    "# Modelo de chat utilizado foi o mistral small, modelo open source da mistral ai\n",
    "if not os.environ.get(\"MISTRAL_API_KEY\"):\n",
    "  os.environ[\"MISTRAL_API_KEY\"] = getpass.getpass(\"Enter API key for Mistral AI: \") # Se a chave não for encontrada o usuário deve inserir\n",
    "\n",
    "llm = init_chat_model(\"mistral-small-2503\", model_provider = \"mistralai\") # Modelo de chat utilizado para a aplicação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d7f531-73b1-442b-92a5-9c533c8b246a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T23:31:54.606464Z",
     "start_time": "2025-06-30T23:30:17.041703Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Embedding model open-source do huggingface\n",
    "embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76efa7-f47f-4a37-94f6-def3bd8fc71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector store chroma, database por diretório\n",
    "vector_store = Chroma( \n",
    "    collection_name=\"exames_extraidos\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"C:/JupyterNotebook/RAG/rag_chroma_db\",  # Caminho do diretório que vai conter os embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bee90f-9e81-4de9-81a2-3c687a78b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando documentos em pdf por diretório\n",
    "directory_path = (\n",
    "    \"C:/JupyterNotebook/RAG/RAG_exames\"\n",
    ")\n",
    "loader = PyPDFDirectoryLoader(\"RAG_exames/\")\n",
    "\n",
    "docs = loader.load() # Carrega os documentos que o RAG vai usar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a45c7a-5050-4b11-997b-13a72eb78526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividindo os documento em chunks para fazer os embeddings\n",
    "text_splitter = RecursiveCharacterTextSplitter(  # Divide os documentos em chunks para ser feito o embedding e guardar na base de dados\n",
    "    chunk_size=1000,  # tamanho do chunk (caracteres)\n",
    "    chunk_overlap=200,  # overlap do chunk (caracteres)\n",
    "    add_start_index=True,  # acompanhar o indice do documento original\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601b04f0-f241-4dfa-9877-864c7f44bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing documents\n",
    "document_ids = vector_store.add_documents(documents=all_splits) # Coloca os documentos no vetor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80349158-de62-4f53-8733-102304f806d3",
   "metadata": {},
   "source": [
    "INICIO DO RETRIEVAL E GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a7927-d6b1-4654-9d81-d3f447136507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Você é um especialista em análise de relatórios de mamografia. Por favor, leia o relatório quando indicado e extraia as seguintes informações:\n",
    "Cisto:\n",
    "- Presente ou Ausente\n",
    "- Localização e tamanho do cisto\n",
    "Nódulo:\n",
    "- Presente ou Ausente\n",
    "- Localização e tamanho do nódulo\n",
    "Calcificação:\n",
    "- Presente ou Ausente\n",
    "- Localização e tamanho da calcificação\n",
    "Microcalcificação:\n",
    "- Presente ou Ausente\n",
    "- Localização e tamanho da microcalcificação\n",
    "BI-RADS: [valor]\n",
    "Outras citações a avaliar: [observações adicionais relevantes]\n",
    "\n",
    "Caso não encontre alguma informação que se encaixe, coloque [sem referência no texto].\n",
    "\n",
    "Diretrizes de Interpretação\n",
    "\n",
    "Diferenciação entre Nódulo e Cisto:\n",
    "\n",
    "Se um achado é identificado inicialmente como \"nódulo\" na mamografia, mas confirmado como \"cisto\" no ultrassom, classifique apenas como CISTO (presente).\n",
    "Nódulos são estruturas sólidas; cistos são estruturas predominantemente líquidas.\n",
    "Complexos sólido-císticos devem ser reportados em ambas categorias (nódulo E cisto).\n",
    "\n",
    "\n",
    "Priorização de Achados Múltiplos:\n",
    "\n",
    "Quando houver múltiplos cistos/nódulos, reporte TODOS, priorizando:\n",
    "a) Achados classificados como suspeitos pelo relatório\n",
    "b) Achados de maior tamanho\n",
    "c) Achados com características atípicas mencionadas\n",
    "\n",
    "\n",
    "Diferenciação entre Calcificações e Microcalcificações:\n",
    "\n",
    "Calcificações: estruturas maiores, geralmente descritas como \"grosseiras\", \"distróficas\", \"vasculares\"\n",
    "Microcalcificações: estruturas menores, frequentemente descritas como \"puntiformes\", \"pleomórficas\", \"lineares\", \"agrupadas\", \"em cluster\"\n",
    "Se o relatório mencionar \"microcalcificações\", classifique especificamente como microcalcificações\n",
    "Se mencionar apenas \"calcificações\", classifique como calcificações.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754230c01b5fb7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# estado para o langgraph\n",
    "class State(NamedTuple):  # Informações que o pipeline vai carregar de um nó para o outro. Facilita leitura, manutenção e evita bugs\n",
    "    question: str  # Pergunta feita pelo usuário\n",
    "    context: Tuple[Document, ...] # Contexto recuperado da base de dados\n",
    "    answer: str  # Resposta gerada pela llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ccb811c0b2d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funções do rag\n",
    "def retrieve(state: State):  # Recupera nos documentos dados parecidos com os inputados\n",
    "    retrieved_docs = vector_store.similarity_search(state.question) # Faz uma comparação da pergunta com os dados no vetor e recupera os mais parecidos\n",
    "    return State(\n",
    "        question=state.question, \n",
    "        context=tuple(retrieved_docs), # Usamos tuple em vez de lista por ser um estrutura imutável, o que facilita o uso do LangGraph\n",
    "        answer=state.answer\n",
    "    )\n",
    "\n",
    "def generate(state: State):  # Gera uma resposta com a adição das informações encontradas no vetor\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state.context)\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state.question, \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return State(\n",
    "        question=state.question,\n",
    "        context=state.context,\n",
    "        answer=response.content\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae2304570507f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O LangGraph vai garantir que cada etapa (nó) do fluxo receba e devolva um objeto do tipo State\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f593be6c-44f5-474c-b515-a2466e871e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_inicial = State(  # Estado incial da aplicação necessário por causa do uso de tuple\n",
    "    question=\"\"\"Faça a extração das características do seguinte exame mamográfico.\n",
    "\n",
    "MAMOGRAFIA DIGITAL DR* BILATERAL\n",
    "\n",
    "Indicação clínica: 69 anos. Rotina. Antecedente de neoplasia mamária.\n",
    "\n",
    "Exame com MAMÓGRAFO DIGITAL nas incidências craniocaudal e mediolateral oblíqua acrescido de incidências em ambas projeções obtidas com manobras de deslocamento posterior dos implantes mamários.\n",
    "\n",
    "Status pós cirurgia conservadora da mama esquerda.\n",
    "Parênquima mamário heterogeneamente denso, o que reduz a sensibilidade da mamografia.\n",
    "Alterações arquiteturais, relacionadas à mamoplastia.\n",
    "Nódulo denso de contornos espiculados projetado no QSE da mama esquerda, associado a retração cutânea,\n",
    "com correspondência ao ultrassom, maior em relação ao exame de 01/2024. Prosseguir com core biopsy.\n",
    "Cisto oleoso na mama esquerda.\n",
    "Calcificações esparsas.\n",
    "Ausência de microcalcificações pleomórficas agrupadas ou ramificadas.\n",
    "Implante bilateral, sem sinais de roturas extracapsulares.\n",
    "Linfonodo axilar, de aspecto reacional.\n",
    "\n",
    "ACR-BIRADS® categoria 5.\"\"\",\n",
    "    context=tuple([]),\n",
    "    answer=\" \"\n",
    ")\n",
    "\n",
    "response = graph.invoke(state_inicial)\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8b959-750d-4e32-af93-77c4bd64cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contexto recuperado da base de dados do RAG\n",
    "print(f'Context: {response[\"context\"]}\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef1dfa-9947-49b5-9e28-9d9f641c99f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph pra acompanhar o RAG\n",
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab9f01-1742-45d7-b259-42616c557644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69420902-7e85-462d-930c-957ff6d8c942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
